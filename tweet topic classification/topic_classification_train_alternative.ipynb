{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet topic classification alternative approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read and preprocess data. Distribute data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data\\\\business.txt', 'data\\\\entertainment.txt', 'data\\\\health.txt', 'data\\\\politics.txt', 'data\\\\sports.txt', 'data\\\\technology.txt']\n"
     ]
    }
   ],
   "source": [
    "filenames = glob.glob(\"data/*.txt\")\n",
    "print(filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract tweets combined with their labeled target and shuffle them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['going', 'sports', 'let', 'win', 'team', 'individual', 'blame', 'championship', 'national', 'tennis', 'golf', 'rest'], 'sports'), (['mark', 'wired', 'spaces', 'world', 'is', 'facebook', 'why', 'bought', 'exactly', 'bizarre', 'oculus', 'zuckerberg', 'the', 'glorious', 'vr'], 'technology'), (['indias', 'champions', 'trophy', 'answered', 'faqs', 'participation'], 'sports'), (['looking', 'music', 'videoclip', 'particular'], 'entertainment'), (['winner', 'pick', 'stock', 'month', 'return', 'results', 'short', 'april', 'market', 'the', 'ehs4290', 'gbt', 'contest'], 'business')]\n"
     ]
    }
   ],
   "source": [
    "labled_tweets = []\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "for filename in filenames:\n",
    "    file = open(filename, encoding='utf-8').read()\n",
    "    for tweet in file.split('\\n'):\n",
    "        tweet = re.sub(r'[^\\w\\s]','', tweet)\n",
    "        tweet = re.sub(\" \\d+\", \" \", tweet)\n",
    "        tweet = [i.lower() for i in list(set(nltk.word_tokenize(tweet)) - set(stopwords))]\n",
    "        labled_tweets.append((tweet, filename[5:-4]))  # extract target names from filename\n",
    "\n",
    "random.shuffle(labled_tweets)\n",
    "print(labled_tweets[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Break it down into a list of tweets and a list of numerical targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['going', 'sports', 'let', 'win', 'team', 'individual', 'blame', 'championship', 'national', 'tennis', 'golf', 'rest']\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "data = [entry[0] for entry in labled_tweets]\n",
    "\n",
    "targets = []\n",
    "for entry in labled_tweets:\n",
    "    t = entry[1]\n",
    "    if t == 'business': \n",
    "        targets.append(1)\n",
    "    if t == 'entertainment':\n",
    "        targets.append(2)\n",
    "    if t == 'health':\n",
    "        targets.append(3)\n",
    "    if t == 'politics':\n",
    "        targets.append(4)\n",
    "    if t == 'sports':\n",
    "        targets.append(5)\n",
    "    if t == 'technology':\n",
    "        targets.append(6)\n",
    "\n",
    "print(data[0])\n",
    "print(targets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 400\n",
    "train_data = data[n:]\n",
    "test_data = data[:n]\n",
    "train_targets = targets[n:] \n",
    "test_targets = targets[:n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2733, 8301)\n"
     ]
    }
   ],
   "source": [
    "count_vect = CountVectorizer(tokenizer=lambda doc: doc, lowercase=False)\n",
    "train_vect = count_vect.fit_transform(train_data)\n",
    "test_vect = count_vect.transform(test_data)\n",
    "print(train_vect.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "252"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect.vocabulary_.get(u'algorithm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2733, 8301)\n"
     ]
    }
   ],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "train_tfidf = tfidf_transformer.fit_transform(train_vect)\n",
    "test_tfidf = tfidf_transformer.transform(test_vect)\n",
    "print(train_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "MNB_classifier = MultinomialNB().fit(train_tfidf, train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "SGD_classifier = SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3,\n",
    "                               random_state=42, max_iter=5, tol=None).fit(train_tfidf, train_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72.25\n"
     ]
    }
   ],
   "source": [
    "predicted = MNB_classifier.predict(test_tfidf)\n",
    "print(np.mean(predicted == test_targets) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76.5\n"
     ]
    }
   ],
   "source": [
    "predicted = SGD_classifier.predict(test_tfidf)\n",
    "print(np.mean(predicted == test_targets) * 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
